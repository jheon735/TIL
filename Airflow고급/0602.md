# 구글 시트 연동
- 구글 시트를 redshift로 옮겨올 수 있음
- 많은 사람들이 구글 시트를 사용하기 때문에 유용함
- 방법
    1. 구글 시트 API 활성화
    2. 구글 서비스 어카운트 생성 후 JSON파일로 다운로드
    3. 어카운트에서 생성한 이메일을 조작하고 싶은 시트에 공유
    4. Airflow DAG에서 JSON파일로 인증하고 앞서 시트를 조작

# Airflow API 사용
- API 활성화
    - airflow.cfg의 api 센션에서 auth_backend 값 변경
    - `[api]` 아래 `auth_backend = airflow.api.auth.backend.basic_auth` 설정
    - docker-compose.yaml에 이미 설정 되어 있음
- 주의사항
    - VPN을 사용
    - 계정 변경
- 사용자 추가
    - Web UI > Security > List Users > 사용자 정보 추가
- Health API 호출
    - API 활성화 여부와 상관 없이 사용 가능
    - airflow 상태를 확인할 수 있음
    - `curl -X GET --user "monitor용id:monitor용PW" http://localhost:8080/health`로 응답 확인
    - 정상인 경우 metadatabase, scheduler의 상태를 json 형태로 리턴
- https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#section/Authentication 에서 다양한 API 사용 확인 가능
- 특정 DAG Trigger하기
    - `http://localhost:8080/api/v1/dags/HelloWorld/dagRuns`가 entrypoint
    - `curl -X POST --user "id:pw" -H 'Content-Type: application/json' -d '{"execution_date":"2023-05-24T00:00:00Z"}' "entrypoint url"`
- 모든 DAG 리스트하기
    - `http://localhost:8080/api/v1/dags`가 entrypoint
    - `curl -X GET --user "id:pw" "entrypoint url"`
- 모든 Variable 리스트 하기
    - `http://localhost:8080/api/v1/variables`가 entrypoint
    - `curl -X GET --user "id:pw" "entrypoint url"`
- 모든 Config 리스트 하기
    - `http://localhost:8080/api/v1/config`가 entrypoint
    - `curl -X GET --user "id:pw" "entrypoint url"`
    - airflow.cfg파일에서 webserver 섹션의 expose_config를 true로 설정해야 함
    - docker-compose.yaml에서 enviroonment에 `AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'`입력 (언더바 2개 확인)

# Dag Dependencies
- DAG 실행 방법
    - 주기적 실행 : schedule 지정
    - 다른 DAG에 의해 트리거
        - Explicit Trigger : Dag A가 분명하게 Dag B를 트리거 (TriggerDagOperator)
        - Reactive Trigger : Dag B가 Dag A가 끝나기를 대기 (ExternalTaskSensor), A가 수정되거나 삭제되면 B가 작동하지 않기 때문에 잘 사용하지 않음
- 태스크 실행 방식
    - 조건에 따라 다른 태스크로 분기(BranchPythonOperator)
    - 과거 데이터 Backfill시 불필요한 태스크 처리 (LatestOnlyOperator)
    - 앞의 태스크가 실패해도 동작해야 하는 태스크 존재
- Explicit Trigger
    - `from airflow.operators.trigger_dagrun import TriggerDagRunOperator` 사용
    - task_id : 태스크 이름
    - trigger_dag_id : 트리거 대상이 되는 dag id
    - conf : 트리거 Dag에 넘기고 싶은 정보 딕셔너리, 트리거 대상 Dag에서는 `{{ dag_run.conf["path"] }}`로 접근, PythonOperator에서는 `kwargs['dag_run'].conf.get('key값')`으로 접근
    - execution_date : 트리거 dag에 넘길 날짜
    - reset_dag_run : True일 때 트리거 dag가 실행 기록이 있더라도 다시 실행
    - wait_for_completion : True일 때 트리거 대상 DAG가 다 끝날 때 까지 기다림
- Reactive Trigger
    - `from airflow.sensors.external_task import ExternalTaskSensor` 사용
    - 동일한 schedule_interval 사용
    - 태스크들의 Execution Date가 동일해야 매칭이 됨
- Jinja Template
    - Python에서 널리 사용되는 템플릿 엔진
    - Django 템플릿 엔진에서 영감을 받아 개발
    - Jinja를 사용하면 프레젠테이션 로직과 애플리케이션 로직을 분리하여 동적으로 HTML 생성
    - Flask에서 사용
    - 변수를 이중 중괄호`{{}}`로 감싸서 사용
    - 제어문은 퍼센트 기호`{% %}`로 표시
    - execution date 같은 task 실행할 때의 파라미터를 레퍼런스 할 수 있음
    - https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html 에서 airflow 변수 확인 가능
    - BashOperator에서 bash_command에서 사용 가능, params에서 dictionary 형태로 레퍼런스 가능
    - Operator 레퍼런스에 templated라고 되어있는 변수에 사용 가능
- Sensor
    - 특정 조건이 충족될 때 까지 대기하는 Operator
    - 외부 리소스 가용성이나 requests 응답 같은 상황 동기화에 유용
    - mode 파라미터가 poke일 땐 worker 하나를 붙잡고 poke 한 후 슬립
    - mode 파라미터가 reschedule일 땐 worker 하나 잡아서 poke한 후 worker release. worker를 다시 못 잡을 수 있음
    - FileSensor : 지정된 위치에 파일이 생길 떄까지 대기
    - HttpSensor : HTTP 요청을 수행하고 지정된 응답이 올 때 까지 대기
    - SqlSensor : SQL DB에 특정 조건이 충족할 때까지 대기
    - TimeSensor : 특정 시간 도달할 때까지 대기
    - ExternalTaskSensor : 다른 Airflow DAG의 특정 작업 완료를 대기 
- BranchPythonOperator
    - `from airflow.operators.python import BranchPythonOperator` 사용
    - 상황에 따라 뒤에 실행되어야 할 태스크를 동적으로 결정해주는 오퍼레이터
    - TriggerDagOperator 앞에 함께 사용하는 경우가 있음
- LatestOnlyOperator
    - `from airflow.operators.latest_only import LatestOnlyOperator` 사용
    - 시인성이 중요한 태스크들이 과거 데이터의 backfill시 실행되는 것을 막기 위함
    - 현재시간이 지금 태스크가 처리하는 execution_date보다 미래이고 다음 execution_date보다 과거인 경우에만 뒤로 실행을 이어가고 아닐 떄 중단
- Trigger Rules
    - `from airflow.utils.trigger_rule import TriggerRule` 사용
    - Operator에 trigger_rule이란 파라미터로 결정 가능
    - 기본값이 all_success인데 이 경우 앞의 task가 성공해야 뒤 task가 실행 됨
    - all_failed는 전부 실패일 때 실행
    - all_done은 성공 실패 여부 없이 끝나고 실행
    - one_failed는 하나라도 실패하는 경우 실행
    - one_success는 하나라도 성공하는 경우 실행
    - none_failed는 앞의 task가 다 성공하거나 다 skipped 될 때 실행
    - none_failed_min_one_success는 하나만이라도 성공하면 실행

# Airflow  메타데이터 DB 내용 보기
- airflow webwerver shell 스크립트 접속
- `psql -h postgres`로 db 접속
- `\dt`로 테이블 목록 확인
- `SELECT * FROM dag_run`으로 DAG 실행 기록 확인

# Task 그룹핑
- `from airflow.utils.task_group import TaskGroup`으로 사용
- 연관성 있는 task를 묶어 처리
- taskgroup 안에 taskgroup을 nesting가능
- taskgroup도 태스크처럼 실행 순서 정의 가능
- with 문으로 task그룹을 사용

# Dynamic Dag
- Jinja template을 기반으로 DAG의 템플릿을 디자인하고 YAML을 통해 템플릿에 파라미터 제공
- 비슷한 DAG를 매뉴얼하게 개발하는 것을 방지
- DAG를 계쏙 만드는 것과 한 DAG안에서 태스크를 늘리는 것 사이의 밸런스 필요
- `from jinja2 import Environment, FileSystemLoader`와 `import yaml`를 활용