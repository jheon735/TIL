# Airflow 설치
- 직접 설치하고 운영
    - 도커 설치 후 Airflow설치
    - AWS EC2 등의 리눅스 서버에 직접 설치
- 클라우드 사용 (프로덕션 환경에서 선호)
    - AWS: MWAA(Managed Workflows for Apache Airflow)
    - 구글 클라우드 : Cloud Composer
    - Microsoft Azure : Azure Data Factory - Airflow DAGs

# Airflow 코드 기본 구조
- DAG 객체를 먼저 만듦
    - 이름
    - 실행주기
    - 실행날짜
    - 오너 등
- DAG 구성하는 태스크 만듦
    - 태스크별 오퍼레이터 선택
    - 태스크 ID를 부여하고 작업의 세부사항 지정
- 태스크들간의 실행 순서 결정
- default_args
    - 딕셔너리 형태로 존재
    - `'retries'` : 실패시 재시도 할 횟수
    - `'retry_delay'` : 재시도 간격
    - `on_failure_callback` : 실패시 수행할 함수
    - `on_success_callback` : 성공했을 때 수행할 함수
- DAG
    - 가장 처음은 DAG 이름
    - `start_date` : 시작날짜, DAG의 시작날짜가 아니라 처음 읽어오는 자료의 시작날짜로 하루 전으로 지정해야함
    - `schedule` : clone 설정 할 때 시간 설정, None, @once, @hourly, @daily, @weekly, @monthly, @yearly로 설정 가능
    - `tags` : 태그
    - `default_args` : 위에서 만든 default arguments
    - `catchup` : True로 설정하면 시작일이 과거일 때 시작일과 지금까지 실행되지 않은 DAG를 모두 실행, False이면 과거 지나간 스케쥴은 수행하지 않음
    - `max_active_runs` : 한번에 동시에 실행될 수 있는 DAG의 수, backfill을 할 때 의미 있음
    - `max_active_tasks` : 한번에 동시에 실행될 수 있는 task의 수

# 실행 방법
- 웹 UI에서 재생 버튼 클릭
- Docker 환경 쉘 스크립트 접속 방법
    1. docker ps로 실행중인 docker 목록 확인
    2. airflow-scheduler의 NAMES로 ID확인
    3. `docker exec -it ID sh`로 shell 띄우기
    4. `docker exec --user root -it ID sh`로 root계정 사용 가능
- Docker 환경에서 새로운 파이썬 모듈 설치 방법
    - 위의 방법으로 docker shell 접속
    - pip3 명령어 사용
- `airflow dags list` : dag 목록 확인
- `airflow tasks list DAG이름` : 특정 DAG의 task 목록
- `airflow tasks test DAG이름 Task이름 날찌` : 특정 task를 수행하나 메타 DB 로그에 기록을 남기지 않음
- `airflow tasks test DAG이름 Task이름 날찌` : 특정 task를 수행하나 메타 DB 로그에 기록을 남김
