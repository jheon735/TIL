# Redshift
- AWS에서 만든 클라우드 기반 DW
- 160GB - 2PB의 데이터까지 처리 가능
- 저용량의 경우 SDD를 사용하여 속도가 빠름
- Still OLAP : 응답속도가 빠르지 않음
- 컬럼 기반 스토리지 : 레코드 별 저장이 아니라 컬럼별로 저장, 컬럼별 압축 가능, 컬럼 추가 삭제가 빠름
- 별크 업데이트 지원 : 레코드가 들어있는 파일을 S3로 복사 후 COPY 커맨드로 Redshift로 일괄 복사
- 고정 용량/비용 SQL 엔진이었으나 Redshift Serveless라는 가변 비용 옵션도 제공
- 데이터 공유 기능(Datashare) : 다른 AWS 계정과 특정 데이터 공유 가능
- Primary key uniqueness를 보장하지 않음
- Postgresql 8.x를 지원하는 툴이나 라이브러리로 접근 가능 : JDBC/ODBC
- Sacle Out : 서버를 한대 더 추가
- Scale Up : 사양을 업그레이드
- 두 대 이상의 노드로 구성되면 분산 저장에 대한 최적화는 개발자가 직접 수행해야 해서 복잡함
- Big Query나 Snowflake는 알아서 관리함
- 데이터 Skew : 데이터를 노드에 나눠 정리할 때 균형있게 저장을 하지 못해 데이터 쏠림이 발생하는 현상
- 레코드의 분배와 저장 방식
    - Diststyle : 레코드의 분배가 어떻게 이뤄지는지 결정
        - all : 모든 레코드에 전부 저장
        - evne : 라운드로빈 형태로 노드별로 균등하게 저장(디폴트)
        - key : 특정 컬럼의 값을 기준으로 노드별로 분배. Primary key를 주로 기준으로 함
    - Distkey : Diststyle이 key일 때 어떤 컬럼을 기준으로 할지 설정
    - Sortkey : 정렬 기준을 나타내는 컬럼
    - SQL 예시 : `CREATE TABLE my_table(column1 INT, ...) DIISTSTYLE KEY DISTKEY(column1) SORTKYE(column2);`
- Redshift의 벌크 업데이트 방식 
    1. 소스에서 데이터 추출
    2. S3에 업로드 (Parquet 포맷 선호)
    3. COPY SQL로 S3에서 Redshift 테이블로 한번에 복사
- 데이터 타입 주의사항 : CHAR, VARCHAR에서 숫자를 사용할 때 글자수를 입력하지만 Redshift는 바이트 단위이므로 한중일어는 한 문자에 3byte 임을 주의

# 일반적인 DW 스키마 구성
- raw_data : ETL 결과
- analytics : ELT 결과
- adhoc : 테스트용 테이블
- pii : 개인정보가 들어감

# Redshift SQL
- 스키마 생성 : `CREATE SCHEMA 스키마이름;`
- 모든 스키마 확인 : `select * from pg_namespace;`
- 사용자 생성 : `CREATE USER id PASSWORD '...';`
- 사용자 확인 : `select * from pg_user;`
- 그룹 생성 : `CREATE GROUP 그룹이름;`
- 그룹에 사용자 추가 : `ALTER GROUP 그룹이름 ADD USER 사용자이름;`
- 그룹에 권한 부여 : `GRANT`
- 그룹 확인 : `SELECT * FROM pg_group;`
- 역할은 계승 구조를 만들 수 있음
- 역할 생성 : `CREATE ROLE 역할이름;`
- 역할 계승 : `GRANTE ROLE 역할이름 TO ROLE 다른역할;`
- 역할 부여 : `GRANTE ROLE 역할이름 TO 사용자;`
- 역할 확인 : `SELECT * FROM SVV_ROLES;`

# Redshift COPY 방법
1. 테이블 생성
    - `CREATE TALBE 스키마.테이블이름 (column1 형태, ...);`
2. CSV 파일을 S3에 업로드
    - S3 버켓 생성
    - 생성된 버켓에 파일 업로드
3. Redshift가 S3에 접근할 수 있도록 접근권한 부여
    - IAM을 이용해 역할을 만들고 Redshift에 부여
4. COPY 명령을 사용해 데이터 적재
    - CSV파일의 경우 delimiter : ','
    - 문자열이 따옴표로 둘러싸인 경우 제거하기 위해 removequotes 지정
    - 헤더를 무시하기 위해 "IGNOREHEADER 1" 지정
    - IAM ROLE 키를 입력해주기 위해 credentials 뒤에 ARN을 읽어와야함
    - `COPY table_name FROM 's3://경로/파일' credentials 'aws_iam_role-anr:aws:iam:xxxx' delimiter ',' datefromat 'auto' timeformat 'auto' IGNOREHEADER 1 removequotes;`
    - `SELECT * FROM stl_load_errors ORDER BY starttime DESC;`로 에러를 확인할 수 있음 