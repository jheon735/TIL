# 데이터 웨어하우스
- 회사에 필요한 모드 ㄴ데이터를 모아놓은 중앙 데이터베이스 (SQL 데이터베이스)
- 데이터의 크기에 맞게 선택
- 크기가 커진다면 다음 중 하나를 선택
    - AWS Redshift, 구글 클라우드의 BigQuery
    - 스노우플레이크
    - 오픈소스 기반의 하둡(Hive/Presto)/Spark
- 프로덕션용 DB와 별개의 DB여야 함
- 다양한 데이터 소스의 예
    - 프로덕션 DB의 데이터
    - 이메일 마케팅 데이터 : Mailchimp, HubSpot, SendGrid...
    - 신용카드 매출 데이터 : Stripe
    - 서포트 티켓 데이터 : Zendesk, Kustomer, ...
    - 서포트 콜 데이터 : ChannelTalk, RingCentral, Talkdesk, ...
    - 세일즈 데이터 : Salesforce
    - 사용자 이벤트 로그 : Amplitude, MixPanel, 웹서버로그, ...

# 데이터 웨어하우스 종류
- AWS Redshift
    - PB 스케일 데이터 분산 처리 가능
    - Postgresql과 호환되는 SQL
    - Python UDF(User Defined Function) 작성을 통해 기능 확장 가능
    - CSV, JSON, Avro, Parquet 등과 같은 다양한 데이터 포맷 지원
    - AWS내 다른 서비스들과 연동이 쉬움
    - 배치 데이터 중심이지만 실시간 데이터 처리 지원
    - 웹 콘솔 이외에도 API를 통한 관리/제어 가능
- Snowflake
    - 외부 고객들에 데이터 판매를 통한 매출을 가능하게 해주는 Data Sharing/Marketplace 제공
    - ETL과 다양한 데이터 통합 기능 제공
    - SQL 기반으로 빅데이터 저장, 처리, 분석을 가능하게 해줌
    - CSV, JSON, Avro, Parquet 등과 같은 다양한 데이터 포맷 지원
    - S3, GC 클라우드 스토리지, Azure Blog Stroage 지원
    - 배치 데이터 중심이지만 실시간 데이터 처리 지원
    - 웹 콘솔 이외에도 API를 통한 관리/제어 가능
- Google Cloud BigQuery
    - 구글 클라우드의 대표 서비스
    - BigQuery SQL사용하여 다른 SQL에 비해 다양한 기능을 제공 (Nested fields, repeated fields 지원)
    - CSV, JSON, Avro, Parquet 등과 같은 다양한 데이터 포맷 지원
    - 구글 클라우드 내 다른 서비스들과 연동이 쉬움
    - 배치 데이터 중심이지만 실시간 데이터 처리 지원
    - 웹 콘솔 이외에도 API를 통한 관리/제어 가능
- Apache Hive
    - Facebook이 시작한 오픈소스 프로젝트
    - 하둡 기반으로 동작하는 SQL 기반 데이터 웨어하우스 서비스
    - HiveQL이라 부르는 SQL 지원
    - MapReduce위에서 동작하는 버전과 Apache Tez를 실행 엔진으로 동작하는 버전 두 가지 존재
    - 다른 하둡 기반 오픈소스들과 연동이 쉬움(Spark, HBase 등등)
    - 자바나 파이썬으로 UDF 작성 가능
    - CSV, JSON, Avro, Parquet 등과 같은 다양한 데이터 포맷 지원
    - 배치 빅데이터 프로세싱 시스템
    - 데이터 파티셔닝과 버키싵ㅇ과 같은 최적화 작업 지원
    - 빠른 처리 속도보다 처리할 수 있는 데이터 양의 크기에 최적화
    - 웹 UI, CLI 두 가지 지원
    - 현재 Spark에 밀리는 분위기
- Apache Presto
    - Facebook의 오픈소스 프로젝트
    - HIVE 보다 처리 속도에 집중 (apahfl rlqks)
    - 다양한 데이터소스에 존재하는 데이터를 대상으로 SQL 실행 가능
    - HDFS, S3, Cassandra, MySQL 등등
    - PrestoSQL 지원
    - CSV, JSON, Avro, ORC, Parquet 등과 같은 다양한 데이터 포맷 지원
    - 웹 UI, CLI 두 가지 지원
    - AWS Athena가 Presto 기반으로 만들어짐
- Apache Iceberg : 데이터 레이크
    - 넷플릭스의 오픈소스 프로젝트
    - 대용랑 SCD(Slowly-Changing Datasets) 데이터를 다룰 수 있는 테이블 포맷
    - HDFS, S3, Azure Blob Storgae등의 클라우스 스토리지 지원
    - ACID 트랜잭션과 타임여행(과거버전으로 롤백과 변경 기록 유지 등)
    - 스키마 진화 지원을 통한 컬럼 제거와 추가 가능
    - 자바와 파이쎤 API 지원
    - Spark, Flink, Hive, Hudi 등의 다른 Apache 시스템과 연동
- Apache Spark : 분산처리 시스템
    - UC 버클리 AMPLab이 2013년 시작한 아파치 오픈소스 프로젝트
    - Python Pandas 방식의 빅데이터 처리를 사용하기 위해 개발
    - 빅데이터 처리 관련 종합 선물세트 : 배치처리(API/SQL), 실시간처리, 그래프 처리, 머신러닝 기능 제공
    - 다양한 분산처리 시스템 지원 : 하둡(YARN), AWS EMR, Google CLoud Dataproc, Mesos, K8s 등
    - 다양한 파일시스템과 연동 : HDFS, S3, Cassandra, HBase 등
    - CSV, JSON, Avro, ORC, Parquet 등과 같은 다양한 데이터 포맷 지원
    - 다양한 언어 지원 : 자바, 파이썬, 스칼라, R

# ETL(Extract, Transform, Load)
- DW 바깥의 데이터를 DW에 로딩하는 작업
- Extract : 외부 데이터를 소스에서 데이터를 추출
- Transform : 데이터의 포맷을 원하는 형태로 변환
- Load : 변환된 데이터를 최종적으로 데이터 웨어하우스로 적재
- 데이터 파이프라인이라고 부르기도 함
- Airflow
    - ETL할 때 많이 쓰이는 프레임웍
    - 파이썬3 기반이며 Aribnb에서 시작
    - AWS, 구글 클라우드에서도 지원
- ETL 관련 SaaS (Software as a Service, 코딩 없이 서비스)도 출현, 흔한 데이터의 경우 Five Tran, Stitch Data 같은 SaaS 사용가능
- 데이터 요약을 위한 ETL이 필요해짐 -> ELT
    - 예시 : 고객 매출 요약 테이블, 제품 매출 요약 테이블 등
    - DBT를 사용하여 주로 진행

# 데이터 시각화
- 시각화 대시보드
    - 중요한 지표를 시간의 흐름과 함께 보여줌
    - 3A : Accessible, Actionalbe, Auditable 이 중요
    - 중요 지표의 예 : 매출액, 월간/주간 액티브 사용자 수, ...
- 많이 사용되는 시각화 대시보드
    - 구글 클라우드 룩커
    - 세일즈포스의 태블로
    - 마이크로노프트의 파워 BI
    - 오픈소스 아파치 수퍼셋

# 데이터 팀
- 데이터 엔지니어
    - 데이터 인프라 구축
    - SW 엔지니어
    - 자바 혹은 스칼라 언어를 아는 것도 좋음
    - DW를 만들고 이를 관리
    - ETL 코드 작성하고 주기적으로 실행
    - Airflow로 ETL 스케쥴러 사용
    - 데이터 분석가, 과학자 지원
    - 대용량 처리 플랫폼 Spark/YARN(하둡)을 사용할 줄 알아야함
    - 컨테이너 기술(Docker, K8s)
    - https://github.com/datastacktv/data-engineer-roadmap 참고
- 데이터 분석가
    - EW의 데이터를 기반으로 지표를 만들고 시각화
    - 태블로(Tableau), 룩커(Looker), 수퍼셋(SZuperset) 많이 사용 됨
    - 내부 직원들의 데이터 관련 질문 응답
- 데이터 과학자
    - 과거 데이터를 기반으로 미래를 예측하는 ML 모델을 만들어 고객들의 서비스 경험 개선
- MLOps
    - DevOps
        - 개발자가 만든 코드를 시스템에 반영하는 프로세스 (CI/CD, deployment)
        - 시스템이 제대로 동작하는지 모니터링 그리고 이슈감지시 escalation 프로세스
    - MLOps
        - DevOps가 하는 일과 동일. 서비스 코드가 아니라 ML 모델이 대상
        - 모델을 계속적으로 빌딩하고 배포하고 성능을 모니터링
        - 데이터 엔지니어링, ML, DevOps를 다 알아야함

# 데이터 레이크
- 구조화 데이터와 비구조화 데이터(로그파일)이 합쳐진 상태
- 보존 기한이 없는 모든 데이터를 원래 형태대로 보존하는 스토리지에 가까움
- DW보다 몇 배는 더 크고 더 경제적인 스토리지
- 클라우드 스토리지, AWS의 S3
- 데이터 레이크가 있는 환경에서 ETL과 ELT
    - ETL : 데이터 레이크와 DW 바깥에서 안으로 데이텨를 가져오는 것
    - ELT : 데이터 레이크와 DW 안에 있는 데이터를 처리하는 것

# 빅데이터 처리 프레임웍
- 분산 환경 기반 (1대 혹은 그 이상의 서버로 구성)
- Fault Tolerance : 소수의 서버가 고장나도 동작해야 함
- 확장이 용이해야 함
- Sacle Out : 서버 추가
- Sacle Up : 각 서버의 사양을 높임
- 1세대 : 하둡 기반의 Mapreduce, Hive/Presto
- 2세대 : Spark (SQL, DataFrame, Streaming, ML, Graph)